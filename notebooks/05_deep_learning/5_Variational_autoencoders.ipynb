{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2318ff14",
   "metadata": {},
   "source": [
    "In this notebook, we will implement a Variational Autoencoder (VAE) using Keras and TensorFlow. The VAE will be trained on the MNIST dataset, which consists of handwritten digits. We will visualize the latent space and generate new samples from the learned distribution.\n",
    "\n",
    "Standard autoencoders map each input to a single point in latent space, which limits their generative capabilities.\n",
    "\n",
    "## Variational Part: Probabilistic Latent Space\n",
    "Instead of mapping each input to a fixed point, a VAE:\n",
    "\t•\tMaps inputs to a distribution (commonly Gaussian) in latent space. For each input, the encoder predicts a mean ($\\mu$) and standard deviation ($\\sigma$) for this distribution.\n",
    "\t•\tSamples a random point from this distribution to pass to the decoder.\n",
    "\n",
    "This approach makes the latent space continuous and smooth, enabling the generation of new—yet realistic—samples by sampling from anywhere within it.\n",
    "\n",
    "## Loss Function\n",
    "The training objective combines two goals:\n",
    "\t•\tReconstruction Loss: Ensures the decoder can faithfully reconstruct the original input from samples in the latent space.\n",
    "\t•\tKL Divergence Loss: Encourages the learned latent distributions to be close to a standard normal distribution, ensuring a well-behaved latent space and enabling generative capabilities.\n",
    "Together, these drive the VAE to both represent the data well and allow for creative sampling.\n",
    "\n",
    "## What Problem Does VAE Solve?\n",
    "\n",
    "Traditional autoencoders can struggle to generate new, realistic samples because their latent space may develop “holes” or regions with no valid representations. VAEs, by enforcing a continuous probabilistic latent space, enable:\n",
    "\t•\tSmooth, meaningful latent representations: Any point in the latent space decodes to a plausible, realistic sample.\n",
    "\t•\tData Generation: New data can be generated by sampling randomly from the latent space.\n",
    "\t•\tInterpolation: It’s possible to smoothly interpolate between data points via the latent space (e.g., morphing one image into another).\n",
    "\n",
    "This makes VAEs very useful for:\n",
    "\t•\tGenerating new images, sounds, or text similar to the training data.\n",
    "\t•\tAnomaly detection (outliers fall outside the learned data distribution).\n",
    "\t•\tData compression and denoising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a963fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.layers import Input, Dense, Flatten, Reshape, Conv2D, Conv2DTranspose, Normalization\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9ce812",
   "metadata": {},
   "source": [
    "## Load the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af433682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Fashion MNIST data\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "x_train = np.pad(x_train, ((0, 0), (2, 2), (2, 2)), mode='constant')\n",
    "x_test = np.pad(x_test, ((0, 0), (2, 2), (2, 2)), mode='constant')\n",
    "x_train = x_train.astype(\"float32\") / 255.\n",
    "x_test = x_test.astype(\"float32\") / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6efe9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\n",
    "    'T-shirt/top',\n",
    "    'Trouser',\n",
    "    'Pullover',\n",
    "    'Dress',\n",
    "    'Coat',\n",
    "    'Sandal',\n",
    "    'Shirt',\n",
    "    'Sneaker',\n",
    "    'Bag',\n",
    "    'Ankle boot'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729fd0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE hyperparameters\n",
    "input_shape = (32, 32, 1)  # Fashion MNIST padded to 32x32\n",
    "latent_dim = 2  # 2D latent space for easy visualization\n",
    "intermediate_dim = 512\n",
    "batch_size = 128\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2889282b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reparameterization trick\n",
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
    "    \n",
    "    Arguments:\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "        \n",
    "    Returns:\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "    z_mean, z_log_var = args\n",
    "    batch = tf.shape(z_mean)[0]\n",
    "    dim = tf.shape(z_mean)[1]\n",
    "    # Sample epsilon from standard normal distribution\n",
    "    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "    # Return sampled latent vector: z = mean + std * epsilon\n",
    "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2b3a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the encoder\n",
    "inputs = Input(shape=input_shape, name='encoder_input')\n",
    "\n",
    "# Convolutional layers for feature extraction\n",
    "x = Conv2D(32, 3, activation='relu', strides=2, padding='same')(inputs)\n",
    "x = Conv2D(64, 3, activation='relu', strides=2, padding='same')(x)\n",
    "x = Conv2D(128, 3, activation='relu', strides=2, padding='same')(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(intermediate_dim, activation='relu')(x)\n",
    "\n",
    "# Generate latent distribution parameters\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "\n",
    "# Use reparameterization trick to sample from latent distribution\n",
    "z = tf.keras.layers.Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "# Instantiate encoder model\n",
    "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f4bf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the decoder\n",
    "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "\n",
    "# Dense layers to expand from latent space\n",
    "x = Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
    "x = Dense(4 * 4 * 128, activation='relu')(x)\n",
    "x = Reshape((4, 4, 128))(x)\n",
    "\n",
    "# Transpose convolutional layers for upsampling\n",
    "x = Conv2DTranspose(128, 3, activation='relu', strides=2, padding='same')(x)\n",
    "x = Conv2DTranspose(64, 3, activation='relu', strides=2, padding='same')(x)\n",
    "x = Conv2DTranspose(32, 3, activation='relu', strides=2, padding='same')(x)\n",
    "\n",
    "# Output layer\n",
    "outputs = Conv2DTranspose(1, 3, activation='sigmoid', padding='same', name='decoder_output')(x)\n",
    "\n",
    "# Instantiate decoder model\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1330f550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom layer to handle KL divergence loss\n",
    "class KLDivergenceLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(KLDivergenceLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
    "        kl_loss = tf.reduce_mean(kl_loss) * -0.5\n",
    "        self.add_loss(kl_loss)\n",
    "        return z_mean, z_log_var\n",
    "\n",
    "# Rebuild the VAE with proper loss handling\n",
    "# Add KL loss layer\n",
    "z_mean_kl, z_log_var_kl = KLDivergenceLayer()([z_mean, z_log_var])\n",
    "\n",
    "# Use the outputs with KL loss added\n",
    "z_with_kl = tf.keras.layers.Lambda(sampling, output_shape=(latent_dim,), name='z_with_kl')([z_mean_kl, z_log_var_kl])\n",
    "\n",
    "# Rebuild encoder with KL loss\n",
    "encoder = Model(inputs, [z_mean_kl, z_log_var_kl, z_with_kl], name='encoder')\n",
    "\n",
    "# Instantiate VAE model with KL loss properly integrated\n",
    "outputs = decoder(encoder(inputs)[2])  # Use the sampled z from encoder\n",
    "vae = Model(inputs, outputs, name='vae_mlp')\n",
    "\n",
    "print(\"VAE Model Architecture:\")\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2476a0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define VAE loss function\n",
    "def vae_loss(inputs, outputs):\n",
    "    \"\"\"VAE loss = reconstruction loss + KL divergence loss\"\"\"\n",
    "    # Reconstruction loss (binary crossentropy)\n",
    "    reconstruction_loss = tf.keras.losses.binary_crossentropy(inputs, outputs)\n",
    "    reconstruction_loss *= input_shape[0] * input_shape[1]  # Scale by image dimensions\n",
    "    \n",
    "    return tf.reduce_mean(reconstruction_loss)\n",
    "\n",
    "# We'll add the KL loss as a separate loss to the model\n",
    "# This is a better approach for VAEs in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98238634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the VAE model\n",
    "# KL loss is automatically added by the KLDivergenceLayer\n",
    "vae.compile(optimizer='adam', loss='binary_crossentropy', metrics=['mae'])\n",
    "\n",
    "print(\"VAE model compiled successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5c8183",
   "metadata": {},
   "source": [
    "## Train the VAE\n",
    "\n",
    "Now we'll train the Variational Autoencoder on the Fashion MNIST dataset. The training process will optimize both the reconstruction quality and the regularization of the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af45d830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for training (add channel dimension)\n",
    "x_train = x_train.reshape(x_train.shape[0], 32, 32, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 32, 32, 1)\n",
    "\n",
    "print(f\"Training data shape: {x_train.shape}\")\n",
    "print(f\"Test data shape: {x_test.shape}\")\n",
    "print(f\"Training data range: [{x_train.min():.3f}, {x_train.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6259cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks for better training\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=5, \n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', \n",
    "    factor=0.5, \n",
    "    patience=3, \n",
    "    min_lr=1e-7\n",
    ")\n",
    "\n",
    "# Train the VAE\n",
    "print(\"Starting VAE training...\")\n",
    "history = vae.fit(\n",
    "    x_train, x_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(x_test, x_test),\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693d92dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('VAE Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['mae'], label='Training MAE')\n",
    "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "plt.title('VAE Training MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45418be0",
   "metadata": {},
   "source": [
    "## Visualize the Latent Space\n",
    "\n",
    "Now let's explore the 2D latent space learned by our VAE. We'll encode the test images and visualize how different fashion items are distributed in the latent space."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
